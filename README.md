# POC Avantages Sportifs ‚Äî Sport Data 
‚ú® Author: Ma√´va Beauvillain üìÖ Start Date: Juin 2025 üìÖ  Last Updated: 15 Juillet 2025

Ce projet constitue un **POC** pour l‚Äôentreprise Sport Data Solution.  
L'objectif est de construire un pipeline de donn√©es permettant :

- Le calcul de la prime sportive des collaborateurs, bas√©e sur leurs modes de d√©placement domicile-travail.
- Le calcul de jours "bien-√™tre" suppl√©mentaires, en fonction de leur pratique sportive r√©guli√®re.

Ce POC vise √† d√©montrer la faisabilit√© technique et √† poser les bases d'une future industrialisation.

---

## Table des mati√®res

1. [Objectif du projet](#1-objectif-du-projet)  
2. [Arborescence](#2-arborescence-du-projet)  
3. [Stack Technique](#3-stack-technique)  
4. [Architecture du projet](#4-architecture-du-projet)  
5. [R√©sultats & KPI](#5-r√©sultats--kpi)  
6. [Instructions de d√©ploiement](#6-instructions-de-d√©ploiement)  

---

## 1. Objectif du projet

Mettre en place un pipeline automatis√© capable de :
- Collecter et transformer des donn√©es RH et sportives
- Calculer automatiquement les avantages √©ligibles (prime / jours)
- Garantir la qualit√© des donn√©es inject√©es
- Publier des messages Slack pour cr√©er une dynamique collective
- Visualiser les r√©sultats RH dans Power BI

---

## 2. Arborescence du projet
```markdown
.
‚îú‚îÄ‚îÄ airflow/dags/               # Pipelines Airflow (prime & bien-√™tre)
‚îú‚îÄ‚îÄ docs/                       # Pr√©sentation, cadrage, Power BI, sch√©mas
‚îú‚îÄ‚îÄ great_expectations/         # Suites de validation et checkpoints
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ prime_sportive/         # Scripts li√©s √† la prime (ingestion, distance, calcul)
‚îÇ   ‚îú‚îÄ‚îÄ bien_etre/              # Scripts li√©s aux jours bien-√™tre (g√©n√©ration, validation, calcul)
‚îÇ   ‚îî‚îÄ‚îÄ sql/                    # Script de cr√©ation du sch√©ma PostgreSQL
‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration des services
‚îú‚îÄ‚îÄ Dockerfile.airflow          # Dockerfile pour Airflow
‚îú‚îÄ‚îÄ Dockerfile.consumer         # Dockerfile pour le consumer Kafka
‚îú‚îÄ‚îÄ pyproject.toml              # D√©pendances Python via Poetry
‚îú‚îÄ‚îÄ requirements-*.txt          # D√©pendances s√©par√©es (Airflow, consumer)
‚îî‚îÄ‚îÄ README.md                   # Ce fichier

```

---

## 3. Stack technique

| Domaine                | Outils utilis√©s                                                |
|------------------------|----------------------------------------------------------------|
| D√©veloppement          | Python, Poetry, Docker                                         |
| Orchestration          | Apache Airflow                                                 |
| Tests de donn√©es       | Great Expectations                                             |
| Stockage               | PostgreSQL, Amazon S3                                          |
| API externe            | Google Maps API                                                |
| Streaming              | Redpanda (Kafka compatible)                                    |
| Visualisation          | Power BI                                                       |
| Notification           | Slack                                                          |

---

## 4. Architecture du projet
Le pipeline combine des traitements **batch** et ****streaming**, orchestr√©s avec Airflow et conteneuris√©s via **Docker**.
L‚Äôobjectif est de collecter, transformer, valider et valoriser les donn√©es RH et sportives de mani√®re automatique et s√©curis√©e.

### Flux batch

- **Nettoyage des fichiers RH & sport**  
  ‚Üí Scripts Python pour normaliser les colonnes, typer les donn√©es, et exporter en `.csv`

- **Ingestion dans PostgreSQL**  
  ‚Üí Les donn√©es nettoy√©es sont ins√©r√©es dans la base apr√®s passage d‚Äôun **checkpoint Great Expectations**

- **Calcul des distances domicile-bureau**  
  ‚Üí Utilisation de l‚Äô**API Google Maps** pour calculer les distances en km selon le mode de d√©placement d√©clar√©

- **Calcul des primes sportives**  
  ‚Üí Attribution de 5% de prime si les conditions sont remplies (distance raisonnable et mode actif)

- **Calcul des jours bien-√™tre**  
  ‚Üí Comptabilisation des activit√©s sur 12 mois, attribution de 5 jours si 15 activit√©s valid√©es

- **Restitution via Power BI**  
  ‚Üí Extraction des r√©sultats pour visualisation RH des KPI

### Flux streaming

- **G√©n√©ration d‚Äôactivit√©s sportives simul√©es**  
  ‚Üí Un script Python publie dans un topic **Redpanda** (Kafka-like)

- **Consumer Kafka**  
  ‚Üí Un autre script consomme les √©v√©nements, valide les donn√©es avec **Great Expectations**, puis les ins√®re en base PostgreSQL

- **Publication Slack**  
  ‚Üí En parall√®le, une notification personnalis√©e est automatiquement envoy√©e dans un canal Slack simul√©, pour cr√©er de l‚Äô√©mulation


### Services conteneuris√©s

Tous les composants sont packag√©s avec Docker :

- `airflow` : orchestration des DAGs  
- `redpanda` : broker streaming  
- `consumer` : script de consommation Kafka  
- `postgres` : base relationnelle  
- `great_expectations` : test de qualit√©

> Voir le fichier [`docker-compose.yml`](./docker-compose.yml) pour la configuration compl√®te.

---

## 5. R√©sultats & KPI

Le POC produit plusieurs indicateurs cl√©s visibles dans Power BI :

- Nombre d‚Äôactivit√©s sportives enregistr√©es (par mois / par salari√©)
- Nombre de salari√©s √©ligibles √† la prime sportive
- Montants totaux et moyens des primes vers√©es
- Nombre de jours bien-√™tre attribu√©s
- R√©partition des modes de d√©placement actifs
- Donn√©es consolid√©es par service ou business unit (BU)

> Rapport Power BI disponible dans [`docs/rapport_power_BI.pdf`](./docs/rapport_power_BI.pdf)

---

## 6. Instructions de d√©ploiement

### Pr√©requis :
- Python 3.11+
- Docker + Docker Compose
- Poetry install√© (`pip install poetry`)

### √âtapes d'installation et de lancement :

1. **Cloner le d√©p√¥t**
```bash
git clone <url_du_repo>
cd poc-avantages-sportifs
```
2. **Installer les d√©pendances Python avec Poetry**
``` bash
poetry install
```
3. **Lancer les services Docker**
``` bash
docker-compose up -d --build
```
4. **Acc√©der √† l‚Äôinterface Airflow**
- **URL** : [http://localhost:8080](http://localhost:8080)

5. **Activer les DAGs dans l‚Äôinterface Airflow**
- `pipeline_prime_sportive`
- `pipeline_jours_bien_etre`

6. **Consulter les r√©sultats**
- Les donn√©es trait√©es sont disponibles dans la base **PostgreSQL**
- Les messages d‚Äôactivit√© sont diffus√©s automatiquement dans **Slack** (ou visibles dans les logs du `consumer`)
- Le rapport statique Power BI est consultable dans [`docs/rapport_power_BI.pdf`](./docs/rapport_power_BI.pdf)

---

Projet d√©velopp√© dans le cadre du parcours **Data Engineer** par OpenClassrooms.